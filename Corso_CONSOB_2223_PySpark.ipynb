{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/3394424051/COVID-19/blob/master/Corso_CONSOB_2223_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark\n",
        "La libreria PySpark mette a disposizione su Python l'ambiente per lo sviluppo e l'esecuzione di codice Spark in Python. La libreria pyspark non è disponibile per default, per cui dobbiamo innanzitutto scaricarla ed installarla."
      ],
      "metadata": {
        "id": "oo2WcgsMpiIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT44aAaqa8TK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4130354c-83bf-4684-b165-b23e0c86bae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=627c7d5e5831e8466731683ff7477347213092600ba17b47c194843b24e57064\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una volta scaricato ed installato pyspark, utilizziamo le seguenti istruzioni per farlo partire.\n",
        "Nota importante: il codice scritto per pyspark può funzionare in tre diverse modalità:\n",
        "\n",
        "\n",
        "*   modalità locale, sfruttando eventualmente tutti i processori presenti sul nostro computer\n",
        "*   modalità cluster, sfruttando un sistema distribuito basato su Spark\n",
        "*   modalità yarn, sfruttando un sistema distribuito basato su Hadoop\n",
        "\n",
        "Durante la lezione, utilizzeremo la sola modalità locale (local). Tuttavia, il codice che andremo a scrivere funzionerà identicamente su un sistema distribuito vero e proprio. La modalità di funzionamento è quella indicata alla voce **master** nelle istruzioni che seguono. Nello specificare la modalità local, possiamo anche indicare quanti processori andranno utilizzati. Indichiamo il valore * se vogliamo usarli tutti. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JivFOLdWpw52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "RKo3fLbma_wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una volta eseguito il codice soprastante, Spark è stato lanciato in esecuzione ed è pronto per essere utilizzato.\n",
        "Spark mette a disposizione diverse librerie e diversi approcci per il calcolo distribuito. Noi approfondiremo il caso delle RDD.\n",
        "\n",
        "## RDD\n",
        "Le RDD sono delle strutture dati distribuite disponibili con Spark che consentono di mantenere una collezione di valori, oppure una collezione di coppie (chiave, valore), di taglia estremamente grande utilizzando un approccio distribuito. A grandi linee, sono come dei vettori. Avremo l'impressione di creare e manipolare RDD all'interno della nostra applicazione, in realtà le RDD si trovano altrove, si trovano su Spark. Quello che noi abbiamo sotto gli occhi, in realtà, è soltanto uno *stub*, una sorta di fantoccio che prende ordini da noi per poi girarli a Spark. \n",
        "\n",
        "Vediamo intanto come si crea una RDD. Due le opzioni principali: la creo a partire da una collezione preesistente oppure la creo a partire da un file.\n",
        "\n",
        "Nel primo caso, posso utilizzare il metodo **parallelize** di SparkContext per trasformare una qualsiasi collezione in una RDD."
      ],
      "metadata": {
        "id": "2pLjjfYtrOm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valori = [4,3,7,2,33,44,11]\n",
        "qualcosa = sc.parallelize(valori)"
      ],
      "metadata": {
        "id": "Y5RDpWfFsQlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualcosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtWJohq6sb7F",
        "outputId": "c99895a8-a156-47b6-853e-48cf10ad688f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark offre due strumenti per lavorare con le RDD:\n",
        "* Azioni: processano il contenuto di una RDD e restituiscono all'applicazione driver uno o più valori\n",
        "* Trasformazioni: processano il contenuto di una RDD e restituiscono altre RDD"
      ],
      "metadata": {
        "id": "-5_2zwHnsm9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esempi di azioni\n",
        "\n",
        "# la taglia della RDD\n",
        "qualcosa.count()\n",
        "# l'elemento di valore minimo\n",
        "qualcosa.min()\n",
        "# l'elemento di valore massimo\n",
        "qualcosa.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76XyCux6s8AG",
        "outputId": "44e9097e-cc6c-4c87-ee62-2790a1dfb0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se proviamo ad accedere fisicamente e direttamente al contenuto di una RDD non otteniamo nulla, poiché la RDD (sulla nostra macchina) non contiene alcun dato, i dati sono altrove."
      ],
      "metadata": {
        "id": "jBvcadGptUKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qualcosa[0]"
      ],
      "metadata": {
        "id": "58GqK_U3tNC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nel caso ci fosse bisogno di riavere alcuni o tutti i dati presenti in una RDD, posso riottenerli attraverso le seguenti azioni"
      ],
      "metadata": {
        "id": "UcSGTCP4thqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take mi restituisce i primi n elementi\n",
        "qualcosa.take(3)\n",
        "# collect mi restituisce tutti gli elementi\n",
        "qualcosa.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4ejor9WtoPz",
        "outputId": "9b4a7e3c-b976-43f8-b050-1af76dfff963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 3, 7, 2, 33, 44, 11]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nella realtà, se usiamo Spark e se usiamo le RDD è perché i nostri dataset sono giganteschi, per cui probabilmente nella mia applicazioe non ci sarà abbastanza spazio per contenerli tutti. Per questo motivo, usare il metodo collect non è una buona idea."
      ],
      "metadata": {
        "id": "L6il8I7Gt2G7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ci soffermiamo piuttosto sulle trasformazioni, sono quelle più interessanti, poiché ci consentono di operare in distribuito su big data sfruttando il parallelismo reso possibile dalla presenza di numerosi computer/processori.\n",
        "\n",
        "Per approfondire questo secondo caso, introduciamo innanzitutto l'utilizzo di file esterni. Spark mette a disposizione il metodo **textFile** attraverso il quale posso caricare il contenuto di un file di testo in una RDD in cui ciascun elemento è una stringa corrispondente ad una delle righe del file letto. Nell'esempio sottostante carichiamo il contenuto del file frutta.txt, per distinguere le variabili RDD dalle altre, faccio precedere il loro dal carattere 'd'."
      ],
      "metadata": {
        "id": "vP6h1IaluLlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dFrutta = sc.textFile('frutta.txt')"
      ],
      "metadata": {
        "id": "2H4hz6QbvBNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dFrutta.count()\n",
        "dFrutta.take(3)"
      ],
      "metadata": {
        "id": "rpM0yKuvvaWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problema 1: determinare la frequenza con cui ricorre il nome di ciascun frutto.\n",
        "Lo risolviamo in due passi. Nel primo, accostiamo ad ogni nome di frutto il valore 1 (la sua frequenza).\n",
        "Nel secondo, aggreghiamo coppie che hanno la stessa chiave, sommando le rispettive frequenze."
      ],
      "metadata": {
        "id": "kpDUiYS3vhAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dFrutta.take(3)"
      ],
      "metadata": {
        "id": "1dluICWtvtpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementiamo il primo passo attraverso una trasformazione map, questa processerà ciascun elemento di dFrutta accostando ad esso il valore 1. La logica della funzione, per ogni elemento x in input, è restituire in output la coppia (x,1). E' possibile codificare questa operazione utilizzando le funzioni *lambda* di Python."
      ],
      "metadata": {
        "id": "UEAUODTtv7iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dFrutta1 = dFrutta.map(lambda x: (x,1))"
      ],
      "metadata": {
        "id": "hD4_TFO9wDNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dFrutta1.count()\n",
        "dFrutta1.take(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmvXF-pXwePy",
        "outputId": "41b514fb-8864-4d49-fadd-86853f9e41dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Arance', 1),\n",
              " ('Susine', 1),\n",
              " ('Carote', 1),\n",
              " ('Zucchine', 1),\n",
              " ('Susine', 1),\n",
              " ('Susine', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nel secondo passo dell'algoritmo, aggreghiamo tutte le coppie che presentano la stessa chiave, sommando le rispettive frequenze. Questa trasformazione si implementa in Spark attraverso una funzione *reduce*. In particolare, dovendo aggregare le coppie che condividono la stessa chiave, utilizziamo **reduceByKey**."
      ],
      "metadata": {
        "id": "PRsTQUjtwmGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dFrutta2 = dFrutta1.reduceByKey(lambda x, y:x+y)"
      ],
      "metadata": {
        "id": "hYbWVOBuw2Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dFrutta2.count()\n",
        "dFrutta2.take(8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k6AT0e3xS_M",
        "outputId": "ea8921e1-dce7-4826-e877-4cf31620dae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Arance', 3),\n",
              " ('Carote', 3),\n",
              " ('Zucchine', 1),\n",
              " ('Mele', 2),\n",
              " ('Kiwi', 2),\n",
              " ('Susine', 4),\n",
              " ('Banane', 1),\n",
              " ('Polpelmi', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problema 2:** implementare l'algoritmo di word counting su un file di testo generico."
      ],
      "metadata": {
        "id": "pXAgo_eIx0c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dTesto = sc.textFile('testo.txt')"
      ],
      "metadata": {
        "id": "QwCxywwDx-tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dTesto.count()\n",
        "dTesto.take(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf4D8QGayChN",
        "outputId": "bedaca89-75c5-4510-c98d-0591c402616a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Naso d'Argento\",\n",
              " '(di Italo Calvino)',\n",
              " '',\n",
              " \"C'era una lavandaia che era rimasta vedova con tre figliole. S'ingegnavano tutte e quattro a lavar roba piu' che potevano, ma pativano la fame lo stesso. Un giorno la figlia maggiore disse alla madre: \"]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Da un primo esame, notiamo che in questo caso ciascuna riga contiene numerose parole. Utilizziamo quindi una trasformazione map per estrapolare da questa riga le parole in esse contenute. A differenza di prima, questa map potenzialmente restituisce per ogni valore in input più valori in output. Per gestire questo caso si utilizza una variante di **map** chiamata **flatMap**. Inoltre, la trasformazione che realizzeremo dovrà contenere al suo interno la logica per estrarre da una riga di testo le parole presenti in essa."
      ],
      "metadata": {
        "id": "KKebu4MIyRXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dParole1 = dTesto.flatMap(lambda x: x.split())"
      ],
      "metadata": {
        "id": "mE-1u159y6vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dParole1.count()\n",
        "dParole1.take(20)"
      ],
      "metadata": {
        "id": "gvPVFdeyzH31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Osserviamo che siamo riusciti ad estrapolare le singole parole correttamente, tuttavia ci sono diverse parole da ripulire, perlomeno rimuovendo i segni di punteggiatura e portando tutto in minuscolo. Dal momento che queste trasformazioni sono da applicare ad ogni singola parola e restituiscono, per ciascuna parola un'altra parola, utilizzo una semplice **map** per trasformare tutto in minuscolo ed una semplice **map** per rimuovere i caratteri spuri."
      ],
      "metadata": {
        "id": "WlAcpIsAzXgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dParole2 = dParole1.map(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "anTDyaG-zi1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dParole2.take(20)"
      ],
      "metadata": {
        "id": "LYlelzVMzt7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dParole3 = dParole2.map(lambda x: x.strip('(),.;:-!?'))\n",
        "dParole3= dParole3.filter(lambda x: len(x)>0)"
      ],
      "metadata": {
        "id": "04eyB015z9jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dParole3.take(20)"
      ],
      "metadata": {
        "id": "kAIPBtS20Dji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dParole4 = dParole3.map(lambda x: (x,1))"
      ],
      "metadata": {
        "id": "auwwu-r50RJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dParole5 = dParole4.reduceByKey(lambda x,y: x+y)"
      ],
      "metadata": {
        "id": "SAV0pMiL0VLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dParole5.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRnor8T00bFg",
        "outputId": "fca07bec-d1ba-45c1-d613-ea3c1ce66618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('naso', 4), (\"d'argento\", 4), (\"c'era\", 2), ('lavandaia', 1), ('vedova', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avremmo risolto il problema, tuttavia la domanda che potrebbe sorgere è: qual è la parola più frequente? Possiamo dare una risposta aiutandoci con l'azione **max**, restituisce l'elemento massimo in una RDD. Tuttavia, questa azione restituisce in una RDD di tuple, la tupla la cui chiave è quella più grande. Essendo le chiavi delle stringhe, max restituisce il valore lessicograficamente più alto."
      ],
      "metadata": {
        "id": "fkEmuRB_0hqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dParole5.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7h_W_co0taY",
        "outputId": "9ea72471-bbca-4f11-dcb4-cf8c0287f2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('voglio', 1)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'azione **max**, come molte altre di Spark, può essere riprogrammata in modo da ragionare diversamente dallo standard. In questo caso, avrei bisogno di valutare l'elemento massimo non rispetto alla chiave ma rispetto al valore. Nell'esempio che segue, per ogni tupla x, valuto l'elemento massimo facendo perno su x[1], ossia, la sua frequenza."
      ],
      "metadata": {
        "id": "ILbIP70e1D2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dParole5.max(key=(lambda x: x[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoDPLo_R1Osm",
        "outputId": "2526914e-2322-473f-f93f-409295698ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('che', 6)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Esercizio 10\n",
        "## Punto 1"
      ],
      "metadata": {
        "id": "asLXP2wgBcA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_Tz2rDDKC2I0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contare le parole che contengono la sottostringa 'lo'\n",
        "\n",
        "sottostringa = 'ci'\n",
        "\n",
        "# Soluzione 1\n",
        "dTesto = sc.textFile('testo1.txt')\n",
        "dParole1 = dTesto.flatMap(lambda x: x.split())\n",
        "dParole2 = dParole1.map(lambda x: x.lower())\n",
        "dParole2bis = dParole2.filter(lambda x: sottostringa in x)\n",
        "dParole2bis.count()\n",
        "#dParole3 = dParole2bis.map(lambda x: x.strip('(),.;:-!?'))\n",
        "#dParole3= dParole3.filter(lambda x: len(x)>0)\n",
        "#dParole4 = dParole3.map(lambda x: (x,1))\n",
        "#dParole5 = dParole4.reduceByKey(lambda x,y: x+y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2xSlnVOBbsJ",
        "outputId": "14c8cba3-460a-44dc-ef1d-20ffc485060c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Soluzione 2\n",
        "dTesto = sc.textFile('testo1.txt')\n",
        "dParole1 = dTesto.flatMap(lambda x: x.split())\n",
        "dParole2 = dParole1.map(lambda x: x.lower())\n",
        "dParole3 = dParole2.map(lambda x: x.strip('(),.;:-!?'))\n",
        "dParole3= dParole3.filter(lambda x: len(x)>0)\n",
        "dParole4 = dParole3.map(lambda x: (x,1))\n",
        "dParole5 = dParole4.reduceByKey(lambda x,y: x+y)\n",
        "\n",
        "dParole6 = dParole5.filter(lambda x: sottostringa in x[0])\n",
        "dFrequenze = dParole6.values()\n",
        "conteggio = dFrequenze.reduce(lambda x,y: x+y)\n"
      ],
      "metadata": {
        "id": "rDkfBXetDLse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conteggio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLPcAOq7Dwox",
        "outputId": "b49b9f55-49d3-42e3-9077-6b57d0983cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## punto 2"
      ],
      "metadata": {
        "id": "nvcyf8NzEjQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dTestoA = sc.textFile('testo1.txt')\n",
        "dTestoB = sc.textFile('testo2.txt')\n",
        "\n",
        "dParoleA1 = dTestoA.flatMap(lambda x: x.split())\n",
        "dParoleA2 = dParoleA1.map(lambda x: x.lower())\n",
        "dParoleA3 = dParoleA2.map(lambda x: x.strip('(),.;:-!?'))\n",
        "dParoleA3= dParoleA3.filter(lambda x: len(x)>0)\n",
        "print(dParoleA3.count())\n",
        "\n",
        "dParoleB1 = dTestoB.flatMap(lambda x: x.split())\n",
        "dParoleB2 = dParoleB1.map(lambda x: x.lower())\n",
        "dParoleB3 = dParoleB2.map(lambda x: x.strip('(),.;:-!?'))\n",
        "dParoleB3= dParoleB3.filter(lambda x: len(x)>0)\n",
        "print(dParoleB3.count())\n",
        "\n",
        "dParoleA4 = dParoleA3.map(lambda x: (x, len(x)))\n",
        "max_word_len = dParoleA4.max(key=lambda x: x[1])[1]\n",
        "print(dParoleA4.filter(lambda x: x[1]==max_word_len).collect())\n",
        "\n",
        "dParoleB4 = dParoleB3.map(lambda x: (x, len(x)))\n",
        "max_word_len = dParoleB4.max(key=lambda x: x[1])[1]\n",
        "print(dParoleB4.filter(lambda x: x[1]==max_word_len).collect())\n",
        "\n",
        "dTestoAB = sc.union([dTestoA, dTestoB])\n",
        "print(dTestoAB.count())\n",
        "\n",
        "# Parole presenti in entrambi i file\n",
        "# elenco delle parole per file senza ripetizioni\n",
        "\n",
        "#dParoleAB = dParoleA3.join(dParoleB3)\n",
        "dParoleA5 = dParoleA3.distinct().map(lambda x: (x,1))\n",
        "dParoleB5 = dParoleB3.distinct().map(lambda x: (x,1))\n",
        "dParoleAB = dParoleA5.join(dParoleB5)\n",
        "dParoleAB.take(20)\n",
        "\n"
      ],
      "metadata": {
        "id": "uCqNxbPDEvw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dParoleAB.saveAsTextFile('dParoleAB.txt')"
      ],
      "metadata": {
        "id": "dPHjlfrvImKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}